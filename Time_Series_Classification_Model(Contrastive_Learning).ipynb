{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "gesture_zip_path = 'home/Downloads/Gesture.zip' \n",
    "gesture_extract_path = 'Gesture_data/'\n",
    "\n",
    "# Step 1: Unzip Gesture.zip\n",
    "with zipfile.ZipFile(gesture_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(gesture_extract_path)\n",
    "\n",
    "# Load gesture data assuming files are in PyTorch tensor format\n",
    "gesture_data = {\n",
    "    'train': torch.load(os.path.join(gesture_extract_path, 'train.pt')),\n",
    "    'val': torch.load(os.path.join(gesture_extract_path, 'val.pt')),\n",
    "    'test': torch.load(os.path.join(gesture_extract_path, 'test.pt'))\n",
    "}\n",
    "\n",
    "# Preprocess function for handling missing values and scaling\n",
    "def preprocess_data(data_dict):\n",
    "    preprocessed_data = {}\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    for split, data in data_dict.items():\n",
    "        data_np = data.numpy()\n",
    "        \n",
    "        # Impute missing values with column mean\n",
    "        if np.isnan(data_np).any():\n",
    "            col_means = np.nanmean(data_np, axis=0)\n",
    "            inds = np.where(np.isnan(data_np))\n",
    "            data_np[inds] = np.take(col_means, inds[1])\n",
    "        \n",
    "        # Scale data\n",
    "        data_np = scaler.fit_transform(data_np)\n",
    "        preprocessed_data[split] = torch.tensor(data_np, dtype=torch.float32)\n",
    "    \n",
    "    return preprocessed_data\n",
    "\n",
    "# Apply preprocessing\n",
    "gesture_preprocessed_data = preprocess_data(gesture_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load HAR data for self-supervised learning pre-training\n",
    "har_zip_path = 'path_to_your_HAR.zip'\n",
    "har_extract_path = 'HAR_data/'\n",
    "with zipfile.ZipFile(har_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(har_extract_path)\n",
    "\n",
    "har_data = {\n",
    "    'train': torch.load(os.path.join(har_extract_path, 'train.pt')),\n",
    "    'val': torch.load(os.path.join(har_extract_path, 'val.pt')),\n",
    "    'test': torch.load(os.path.join(har_extract_path, 'test.pt'))\n",
    "}\n",
    "\n",
    "# Self-supervised model using a simple contrastive learning architecture\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim=128):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, feature_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Define contrastive loss\n",
    "def contrastive_loss(features, temperature=0.5):\n",
    "    cos_sim = nn.CosineSimilarity(dim=-1)\n",
    "    labels = torch.arange(len(features)).long()\n",
    "    logits = cos_sim(features.unsqueeze(1), features.unsqueeze(0)) / temperature\n",
    "    return nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "# Training loop for self-supervised learning\n",
    "def pretrain_contrastive_model(har_data, input_dim):\n",
    "    model = ContrastiveModel(input_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    train_loader = DataLoader(har_data['train'], batch_size=32, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            features = model(data)\n",
    "            loss = contrastive_loss(features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Pretrain the model\n",
    "input_dim = har_data['train'].shape[1]  # Assuming 2D input (samples x features)\n",
    "contrastive_model = pretrain_contrastive_model(har_data, input_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        self.encoder = contrastive_model.encoder  # Reuse pre-trained encoder\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Define the training function for supervised learning\n",
    "def train_classifier(model, data, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    train_loader = DataLoader(data['train'], batch_size=32, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Instantiate and train classifier\n",
    "num_classes = 8  # Adjust based on UWaveGestureLibrary class count\n",
    "classifier_model = ClassifierModel(feature_dim=128, num_classes=num_classes)\n",
    "train_classifier(classifier_model, gesture_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions, labels = [], []\n",
    "        for inputs, targets in DataLoader(data['test'], batch_size=32):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.numpy())\n",
    "            labels.extend(targets.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Evaluate the classifier\n",
    "evaluate_model(classifier_model, gesture_preprocessed_data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
